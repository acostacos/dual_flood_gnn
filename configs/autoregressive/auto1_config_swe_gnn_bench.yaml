dataset_parameters:
  storage_mode: 'memory'                                # 'memory' or 'disk'
  root_dir: 'data/datasets'
  nodes_shp_file: 'New_Geometry/cell_centers_with_ele.shp'
  edges_shp_file: 'New_Geometry/links_with_slope.shp'
  features_stats_file: 'features_stats.yaml'

  previous_timesteps: 2
  normalize: True
  timestep_interval: 900                                 # Interval between timesteps in seconds
  spin_up_time: 259200                                    # Spin up time in seconds; Can be defined per Run ID
  # spin_up_time:
  #   38: 259200
  #   39: 259200
  #   40: 259200
  #   41: 259200
  #   default: 129600
  time_from_peak: 7200                                 # Time from peak in seconds; This is used to determine the number of timesteps to consider from the peak
  inflow_boundary_nodes:
  - 1248
  outflow_boundary_nodes:
  - 1253
  - 1129

  training:
    dataset_summary_file: 'train.csv'
    event_stats_file: 'train_event_stats.yaml'

  testing:
    dataset_summary_file: 'test.csv'
    event_stats_file: 'test_event_stats.yaml'

training_parameters:
  log_path             : 'autoregressive_water_depth_52events/logs/auto8_train.log'                                # Path to save training logs
  model_dir            : 'autoregressive_water_depth_52events/model'                      # Directory to save trained models
  stats_dir            : 'autoregressive_water_depth_52events/train'                    # Directory to save training statistics
  checkpoint_path      : 'autoregressive_water_depth_52events\model\NodeGNNAttn_2025-09-04_23-14-49_auto6.pt'                                # Path to load a pre-trained model; If None, no pre-trained model is loaded
  num_epochs           : 100                                 # Number of epochs
  num_epochs_dyn_loss  : 0                                  # Number of epochs to determine loss scaling ratios dynamically; Usually 10 epochs is sufficient
  # num_epochs           : 0                                 # Number of epochs
  # num_epochs_dyn_loss  : 10                                  # Number of epochs to determine loss scaling ratios dynamically; Usually 10 epochs is sufficient
  batch_size           : 64                                  # Batch size
  learning_rate        : 1.18e-5                              # Learning rate
  adam_weight_decay    : 0                                   # Weight decay
  gradient_clip_value : 0.5                                  # Gradient clipping value; If None, no gradient clipping is applied
  early_stopping_patience: 15                                # Early stopping patience for autoregressive training
  val_split_percent  : 0.05                                   # Percentage of validation split for autoregressive training

  autoregressive:
    enabled            : True                                # Use autoregressive training
    init_num_timesteps : 7                                   # Initial number of timesteps for autoregressive training
    total_num_timesteps: 8                                   # Total number of timesteps for autoregressive training
    learning_rate_decay  : 0.7                               # Learning rate decay factor after each curriculum step

loss_func_parameters:
  edge_pred_loss_scale: 2.5e-1                                    # Value used to scale edge prediction loss; This is used as an initial value when using dynamic loss scaling
  edge_pred_loss_percent: 0.7                                # Percentage of edge prediction loss in total loss
  use_global_mass_loss: False                                # Use global mass conservation in loss function
  global_mass_loss_scale: 1.0e-5                             # Value used to scale global mass conservation loss; This is used as an initial value when using dynamic loss scaling
  global_mass_loss_percent: 0.001                            # Percenage of global mass conservation loss in total loss
  use_local_mass_loss: False                                 # Use local mass conservation in loss function
  local_mass_loss_scale: 1.0e-5                              # Value used to scale local mass conservation loss; This is used as an initial value when using dynamic loss scaling
  local_mass_loss_percent: 0.001                             # Percentage of local mass conservation loss in total loss

testing_parameters:
  log_path: 'autoregressive_water_depth_52events/logs/auto8_test.log'                                        # Path to save testing logs
  output_dir: 'autoregressive_water_depth_52events/test'                           # Directory to save testing results
  rollout_start: 0                                      # Start timestep for the test rollout
  rollout_timesteps: null                               # Number of timesteps for the test rollout (30 seconds each)

model_parameters:
  GCN:
    hidden_features: 32
    num_layers: 2
    activation: 'prelu'
    residual: True
    encoder_layers: 2
    encoder_activation: 'prelu'
    decoder_layers: 2
    decoder_activation: 'prelu'

  GAT:
    hidden_features: 32
    use_edge_features: False
    num_layers: 2
    activation: 'prelu'
    residual: True
    num_heads: 1
    dropout: 0.0
    add_self_loops: True
    negative_slope: 0.2
    attn_bias: True
    attn_residual: True
    encoder_layers: 2
    encoder_activation: 'prelu'
    decoder_layers: 2
    decoder_activation: 'prelu'

  NodeEdgeGNN:
    hidden_features: 32
    num_layers: 2
    activation: 'prelu'
    residual: True
    mlp_layers: 2
    encoder_layers: 2
    encoder_activation: 'prelu'
    decoder_layers: 2
    decoder_activation: 'prelu'

  NodeEdgeGNNAttn:
    hidden_features: 16
    num_layers: 2
    activation: 'prelu'
    residual: True
    dropout: 0.0
    negative_slope: 0.2
    attn_mlp_layers: 2
    encoder_layers: 2
    encoder_activation: 'prelu'
    decoder_layers: 2
    decoder_activation: 'prelu'

  NodeGNNAttn:
    hidden_features: 8
    num_layers: 2
    activation: 'prelu'
    residual: True
    dropout: 0.0
    negative_slope: 0.2
    attn_mlp_layers: 2
    encoder_layers: 2
    encoder_activation: 'prelu'
    decoder_layers: 2
    decoder_activation: 'prelu'

  EdgeGNNAttn:
    hidden_features: 8
    num_layers: 2
    activation: 'prelu'
    residual: True
    dropout: 0.0
    negative_slope: 0.2
    attn_mlp_layers: 2
    encoder_layers: 2
    encoder_activation: 'prelu'
    decoder_layers: 2
    decoder_activation: 'prelu'
